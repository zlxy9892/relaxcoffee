### 1 BP 神经网络概述

​	BP 神经网络是一类基于误差逆向传播 (BackPropagation, 简称 BP) 算法的多层前馈神经网络，BP算法是迄今最成功的神经网络学习算法。现实任务中使用神经网络时，大多是在使用 BP 算法进行训练。值得指出的是，BP算法不仅可用于多层前馈神经网络，还可以用于其他类型的神经网络，例如训练递归神经网络。但我们通常说 “BP 网络” 时，一般是指用 BP 算法训练的多层前馈神经网络。

### 2 神经网络的前馈过程

假设我们构造一个经典的神经网络，如下图，该网络可接收输入数据 $x$，包含两个属性 $x_1, x_2$，包含一个隐含层，隐含层的每个神经元首先通过 $w{1}$ 和 $b_1$ (b 为偏置项) 将输入参数进行线性组合得到 $z^{[1]}$，然后将 $z^{[1]}$ 带入激活函数 (activation function) $g(z)$，得到隐含层的一组输出 $a^{[1]}$。接下来，将这组 $a^{[1]}$ 通过 $w^{[2]}$ 和 $b_2$ 进行第二次线性组合，得到 $z^{[2]}$，再带入激活函数 $g(z)$，得到最终的输出值 $a^{[2]}$，该值即为学习器 (广义上看，神经网络属于学习器的一种类型) 的推测结果 $\hat{y}$。

![ann_structure](./pic/ann1.png) 



 	上述过程，即为该神经网络的前馈 (forward propagation) 过程，前馈过程也非常容易理解，符合人正常的逻辑，具体的矩阵计算表达如下：

​	$$z^{[1]}=w^{[1]}x+b^{[1]}$$

​	$$a^{[1]}=g(z^{[1]})$$

​	$$z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}$$

​	$$a^{[2]}=g(z^{[2]})$$

### 3 逆向误差传播 (BP过程)

​	接下来我们就开始基于误差逆传播的神经网络学习过程，到此为止，我们我网络已经可以完成正向的计算，也就是说，给一组 $x$ 的输入，就能够通过网络计算得到推测的结果 $\hat{y}$。然而，当前我们并不知道网络中的参数，网络中的参数主要包含 $w^{[1]}$、$b^{[1]}$、$w^{[2]}$ 和 $b^{[2]}$，激活函数 $g(z)$ 也有可能作为一个未知的参数，但在此我们简单起见，默认该函数为 Sigmoid 函数，即 $g(z)=\frac{1}{1+e^{-z}}$。那么，从机器学习的角度来看，我们的**任务就是：给定了一组数据集，其中包含了输入数据 $X$ 和输出的真实结果 $y$，如何寻找一组最佳的神经网络参数，使得网络计算得到的推测值 $\hat{y}$ 能够与真实值 $y$ 吻合程度最高？**

#### 3.1 模型的损失函数

​	为了达到这个目标，这也就转换为了一个优化过程，对于任何优化问题，总是会有一个目标函数 (objective function)，在机器学习的问题中，通常我们称此类函数为：**损失函数 (cost function)**，具体来说，损失函数表达了推测值与真实值之间的误差。抽象来看，如果把模型的推测以函数形式表达为 $\hat{y}=h(x)$ (这里的函数字母使用 h 是源于“假设 hypothesis“ 这一单词)，那么损失函数则可表达为：

​	$$L=\frac{1}{m}{\sum_{i=1}^{m}{(\hat{y}-y)^2}}$$

​	那么在该问题中的损失函数是什么样的呢？抛开线性组合函数，我们先着眼于最终的激活函数，也就是 $a^{[2]}$ 的值，由于 sigmoid 函数具有很好的函数性质，其值域介于 0 到 1 之间，当自变量很大时，趋向于 1，很小时趋向于 0，因此，该模型的损失函数可以定义如下：

​	$\begin{eqnarray}Cost(a, y)=\begin{cases}-\ln(a), &y=1\cr -\ln(1-a), &y=0 \end{cases}\end{eqnarray}$

​	不难发现，由于是分类问题，真实值只有取 0 或 1 两种情况，当真实值为 1 时，输出值 $a$ 越接近 1，则 cost 越小，当真实值为 0 时，输出值越接近于 0，则 cost 函数越小 (可自己手画一下 $-\ln(x)$ 函数的曲线)。因此，可将该分段函数整合为如下函数：

​	$$L(a, y)=-[ y (\ln(a) + (1-y) \ln(1-a) ]$$

​	该函数与上述分段函数等价。现在，我们已经确定了模型的损失函数关于输出量 $a$ 的函数形式，接下来的问题自然就是：如何根据该损失函数来优化模型的参数。

#### 3.2 基于梯度下降的逆向传播过程 (关于损失函数的逆向求导)

​	这里需要一些先修知识，主要是需要懂得梯度下降 (gradient descent) 这一优化算法的原理，这里我就不展开阐述该方法，不了解的读者可以查阅相关的知识。由于神经网络的损失函数如果完全展开将会即为复杂，这里我们先根据上面得到的关于输出量 $a$ 的损失函数来进行一步步的推导。

​	BP 的核心点在于逆向传播，本质上来说，其实是将模型最终的损失函数进行逆向求导的过程，首先，我们需要从输出层向隐含层的 $W^{[2]}$ 进行求导，也就是需要求得：$$\frac{\partial{L}}{\partial{W^{[2]}}}$$ ，这里就需要用到求导方法里的**链式求导法则**。我们先求得 2 个必要的导数 ( $$\frac{\partial{L}}{\partial{a}}$$ 和 $$\frac{\partial{a}}{\partial{z}}$$ )：

​	$$\frac{\partial{L}}{\partial{a}} = \frac{1-y}{1-a} - \frac{y}{a} = \frac{a-y}{a(1-a)}$$

​	 $$\frac{\partial{a}}{\partial{z}} = a(1-a)$$         ( 这里的求导结果也是 sigmoid 函数的一个特殊性质 ) 

接下来，开始通过链式法则求 $$\frac{\partial{L}}{\partial{z^{[2]}}}$$ ：

​	$$\frac{\partial{L}}{\partial{z^{[2]}}} = \frac{\partial{L}}{\partial{a^{[2]}}} \cdot  \frac{\partial{a^{[2]}}}{\partial{z^{[2]}}} = a-y$$

继续，可求得 $$\frac{\partial{L}}{\partial{w^{[2]}}}$$ ,  $$\frac{\partial{L}}{\partial{b^{[2]}}}$$ ：

​	$$\frac{\partial{L}}{\partial{W^{[2]}}} = \frac{\partial{L}}{\partial{a^{[2]}}} \cdot  \frac{\partial{a^{[2]}}}{\partial{z^{[2]}}} \cdot  \frac{\partial{z^{[2]}}}{\partial{W^{[2]}}} = (a-y)\cdot x$$

​	 $$\frac{\partial{L}}{\partial{b^{[2]}}} = a-y$$

​	至此，我们已经逆向求导得到了损失函数关于隐含层中 $W^{[2]}$ 和 $b^{[2]}$ 这两个参数的导数，根据梯度下降法，可迭代更新这两个参数值：

​	$$W^{[2]} = W^{[2]} - \alpha \frac{\partial{L}}{\partial{W^{[2]}}}$$

​	$$b^{[2]} = b^{[2]} - \alpha \frac{\partial{L}}{\partial{b^{[2]}}}$$

接下来，根据上述求得结果，我们继续逆向传播的过程。首先可求得损失函数关于 $z^{[1]}$ 的导数：

​	$$\frac{\partial{L}}{\partial{z^{[1]}}} = \frac{\partial{L}}{\partial{a^{[2]}}} \cdot  \frac{\partial{a^{[2]}}}{\partial{z^{[2]}}} \cdot \frac{\partial{z^{[2]}}}{\partial{a^{[1]}}} \cdot \frac{\partial{a^{[1]}}}{\partial{z^{[1]}}} = {W^{[2]}}^{T} \cdot \frac{\partial{L}}{\partial{z^{[2]}}} * g'(z^{[1]})$$

其中，$$\frac{\partial{z^{[2]}}}{\partial{a^{[1]}}} = {W^{[2]}}^{T}$$，$$g'(z^{[1]}) = \frac{\partial{a}}{\partial{z}} = a(1-a)$$  ( 即对 sigmoid 激活函数求导 )。

​	求得 $$\frac{\partial{L}}{\partial{z^{[1]}}}$$ 之后，可方便地分别求得输入层至隐含层中 $W^{[1]}$,  $b^{[1]}$ 的导数：

​	$$\frac{\partial{L}}{\partial{W^{[1]}}} = \frac{\partial{L}}{\partial{z^{[1]}}} \cdot x^{T}$$

​	$$\frac{\partial{L}}{\partial{b^{[1]}}} = \frac{\partial{L}}{\partial{z^{[1]}}}$$

同样的，根据梯度下降法对参数 $W^{[2]}$ 和 $b^{[2]}$ 进行更新：

​	$$W^{[1]} = W^{[1]} - \alpha \frac{\partial{L}}{\partial{W^{[1]}}}$$

​	$$b^{[1]} = b^{[1]} - \alpha \frac{\partial{L}}{\partial{b^{[1]}}}$$

至此，我们已经完成了整个逆向传播 BP 的过程。